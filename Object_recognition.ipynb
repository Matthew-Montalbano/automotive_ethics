{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_editable_ID.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1XdU1oQ4SFbxXmE1sc2NI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matthew-Montalbano/automotive_ethics/blob/main/Object_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDEPTdD-Uo7B"
      },
      "source": [
        "Placeholder dataset: https://www.kaggle.com/veeralakrishna/butterfly-dataset\n",
        "Download \"images\" folder from here and upload to your drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaMau2eqaUuo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ec14a110-f60d-493c-c8c8-e75f2a8d090d"
      },
      "source": [
        "#Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS70SUNdt6cp"
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "# Other Modules\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A7clBdtuWpV"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSped5awygt4"
      },
      "source": [
        "#change this to where the \"images\" folder is located on your drive\n",
        "base_directory = \"/content/drive/My Drive/images\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G9DIWWizK1B"
      },
      "source": [
        "The scientific (Latin) names of the butterfly categories are: \n",
        "\n",
        "001: Danaus plexippus\t\n",
        "002: Heliconius charitonius\t\n",
        "003: Heliconius erato\t\n",
        "004: Junonia coenia\t\n",
        "005: Lycaena phlaeas\n",
        "\n",
        "006: Nymphalis antiopa\t\n",
        "007: Papilio cresphontes\t\n",
        "008: Pieris rapae\t\n",
        "009: Vanessa atalanta\t\n",
        "010: Vanessa cardui"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1CrNbHhyqtB"
      },
      "source": [
        "#create labels for classes\n",
        "#classes = ['Danaus plexippus', 'Heliconius charitonius', 'Heliconius erato', 'Junonia coenia', 'Lycaena phlaea', 'Nymphalis antiopa', 'Papilio cresphontes', 'Pieris rapae','Vanessa atalanta', 'Vanessa cardui' ]\n",
        "classes = ['001', '002', '003', '004', '005', '006', '007', '008','009', '010' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CnJaBoj1qT_"
      },
      "source": [
        "#put pictures into correct class folders: uncomment code if running for first time, then comment\n",
        "#images = glob.glob((base_directory) + '/*.png')\n",
        "#for cl in classes:\n",
        "  #images = glob.glob((base_directory) + '/' + cl + '*.png')\n",
        "  #print(\"{}: {} Images\".format(cl, len(images)))\n",
        "  #print(os.path.join(base_directory, cl, cl))\n",
        "  #for i in images:\n",
        "    #shutil.copy(i, os.path.join(base_directory, cl))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmm0De3ZKkWO"
      },
      "source": [
        "#create training, test, validation sets from existing data: uncomment code if running for first time, then comment\n",
        "#for cl in classes:\n",
        "  #print(cl)\n",
        "  #image_path = os.path.join(base_directory,cl)\n",
        "  #print(image_path)\n",
        "  #use glob extension to loop through image files\n",
        "  #images = glob.glob(image_path + '/*.png')\n",
        "  #print(\"{}: {} Images\".format(cl, len(images)))\n",
        "  #from 0 - .7 of images, from .7 - .7+.15 of images, from .7+.15-100 of images\n",
        "  #train,val,test = images[:round(len(images)*0.7)], images[round(len(images)*0.7):round(len(images)*0.85)], images[round(len(images)*.85):]\n",
        "  #print(len(val))\n",
        "  #print(os.path.join(base_directory, 'val', cl))\n",
        "  #for v in val:\n",
        "     #shutil.copy(v, os.path.join(base_directory, 'val', cl))\n",
        "  #print(\"done with \" + cl + \" val\" )\n",
        "  #for ts in test:\n",
        "     #shutil.copy(ts, os.path.join(base_directory, 'test', cl))\n",
        "  #print(\"done with \" + cl + \" test\" )\n",
        "  #for t in train:\n",
        "     #shutil.copy(t,os.path.join(base_directory, 'train', cl))\n",
        "  #print(\"done with \" + cl + \" train\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHohxVB6NYt6"
      },
      "source": [
        "#set up paths to training, validation, and test directories\n",
        "train_dir = os.path.join(base_directory,'train') \n",
        "val_dir = os.path.join(base_directory,'val')\n",
        "test_dir = os.path.join(base_directory,'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IdhJxtQNezl"
      },
      "source": [
        "#Define macro values here\n",
        "BATCH_SIZE = 100  # Number of training examples to process before updating our models variables\n",
        "IMG_SHAPE  = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLhaSGSkNjEh"
      },
      "source": [
        "#rescale RBG values from 0-255 to 0-1\n",
        "train_image_generator      = ImageDataGenerator(rescale=1./255)  # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255)  # Generator for our validation data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isno9VXGNnYD"
      },
      "source": [
        "#Generate batches of tensor image data. We feed in our paths to our images and the generator will \n",
        "#apply our data augmentation functions to batch size chucks for all the classes of our images.\n",
        "# we do data augmentation to prevent overfitting\n",
        "\n",
        "image_gen_train = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=45,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.5\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPiKLQhNtmh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cada88b-cdcf-43fa-d6fa-b39ca55b4eca"
      },
      "source": [
        "train_data_gen = image_gen_train.flow_from_directory(\n",
        "                                                batch_size=BATCH_SIZE,\n",
        "                                                directory=train_dir,\n",
        "                                                shuffle=True,\n",
        "                                                target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                class_mode='sparse'\n",
        "                                                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 582 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCDAyPtCNxe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa722dc3-aa54-4d4e-a7ac-e71c942cb4d8"
      },
      "source": [
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                 directory=val_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 125 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEKaSiUIN4E5"
      },
      "source": [
        "#import regularizer\n",
        "from keras import regularizers\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    #convolution layer 16 outputs to one image input \n",
        "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(150, 150, 3)),\n",
        "    #downsampling image\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    #convolution layer 32 outputs to one image input \n",
        "    tf.keras.layers.Conv2D(32, 3,kernel_regularizer=regularizers.l2(0.01), padding='same', activation='relu'),\n",
        "    #downsampling image\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #convolution layer 64 outputs to one image input \n",
        "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    #downsampling image\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    # flattens to 1d array\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #Dropout of 20% to prevent over fitting (turns off some neurons)\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    \n",
        "    tf.keras.layers.Dense(512, kernel_regularizer=regularizers.l2(0.01), activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bltwQHFeN7mB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "09bdd7a1-6a5b-432b-fd6b-7ef6cd4bb3c3"
      },
      "source": [
        "#compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 150, 150, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 75, 75, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 75, 75, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 37, 37, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 20736)             0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 20736)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               10617344  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 10,646,058\n",
            "Trainable params: 10,646,058\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCWmEVEVOA30"
      },
      "source": [
        "#import callbacks from keras\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#implement callbacks to early stop training and use best model so far\n",
        "early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_checkpoint_monitor = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "#reset variable weights here \n",
        "model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw6ZViEeWDLe"
      },
      "source": [
        "#uncomment to test if all files valid\n",
        "#import os\n",
        "#from PIL import Image\n",
        "#folder_path = train_dir\n",
        "#extensions = []\n",
        "#for fldr in os.listdir(folder_path):\n",
        "    #sub_folder_path = os.path.join(folder_path, fldr)\n",
        "    #for filee in os.listdir(sub_folder_path):\n",
        "        #file_path = os.path.join(sub_folder_path, filee)\n",
        "        #print('** Path: {}  **'.format(file_path), end=\"\\r\", flush=True)\n",
        "        #im = Image.open(file_path)\n",
        "        #rgb_im = im.convert('RGB')\n",
        "        #if filee.split('.')[1] not in extensions:\n",
        "            #extensions.append(filee.split('.')[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3xaCPkGOJhr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "cbd02830-a9f1-4b9c-f331-7fe207e44dcc"
      },
      "source": [
        "#TRAIN MODEL\n",
        "from PIL import Image\n",
        "epochs = 30\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch= int(np.ceil(train_data_gen.n / float(BATCH_SIZE))),\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stopping_monitor, model_checkpoint_monitor], # Early stopping\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(val_data_gen.n / float(BATCH_SIZE)))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "6/6 [==============================] - 28s 5s/step - loss: 1.7262 - accuracy: 0.6289 - val_loss: 1.6172 - val_accuracy: 0.6720\n",
            "Epoch 2/30\n",
            "6/6 [==============================] - 24s 4s/step - loss: 1.7080 - accuracy: 0.6289 - val_loss: 1.5561 - val_accuracy: 0.6800\n",
            "Epoch 3/30\n",
            "6/6 [==============================] - 23s 4s/step - loss: 1.6643 - accuracy: 0.6289 - val_loss: 1.4644 - val_accuracy: 0.7040\n",
            "Epoch 4/30\n",
            "6/6 [==============================] - 25s 4s/step - loss: 1.5286 - accuracy: 0.6907 - val_loss: 1.4733 - val_accuracy: 0.7360\n",
            "Epoch 5/30\n",
            "6/6 [==============================] - 24s 4s/step - loss: 1.5932 - accuracy: 0.6718 - val_loss: 1.4041 - val_accuracy: 0.7520\n",
            "Epoch 6/30\n",
            "6/6 [==============================] - 26s 4s/step - loss: 1.6746 - accuracy: 0.6375 - val_loss: 1.5450 - val_accuracy: 0.6800\n",
            "Epoch 7/30\n",
            "6/6 [==============================] - 24s 4s/step - loss: 1.6523 - accuracy: 0.6667 - val_loss: 1.5260 - val_accuracy: 0.7200\n",
            "Epoch 8/30\n",
            "6/6 [==============================] - 23s 4s/step - loss: 1.6541 - accuracy: 0.6684 - val_loss: 1.5916 - val_accuracy: 0.7280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7CxyyExOkKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "d2414e6b-4a0e-4a3b-a237-7e218d36edf9"
      },
      "source": [
        "model.save('saved_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3d511450d579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMvfXggIO9SL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "3dc4e0ed-b1f2-4dac-ccc1-aa6e33133898"
      },
      "source": [
        "#LOAD SAVED MODEL\n",
        "model = model.load_model('saved_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0307639ba73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#LOAD SAVED MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTWuBC0QHFuI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "897e18c9-75ab-4d5a-89e4-d079f33c472c"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c2fd09281f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpNIf_LuKSE4"
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(early_stopping_monitor.stopped_epoch, acc, label='Training Accuracy')\n",
        "plt.plot(early_stopping_monitor.stopped_epoch, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5c4oFjxJ8Ep"
      },
      "source": [
        "#TEST DIRECTORY LOSS AND ACCURACY\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(directory=test_dir, target_size = (150,150), class_mode='sparse')\n",
        "test_loss, test_accuracy=model.evaluate_generator(test_generator)\n",
        "print(test_loss)\n",
        "print(test_accuracy)\n",
        "\n",
        "predictions = model.predict(img_tensor)\n",
        "print(predictions)\n",
        "np.argmax(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQRVd9LVKWXw"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "img = image.load_img(test_dir + \"\", target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
        "model.predict(img_tensor)\n",
        "\n",
        "img_tensor\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}